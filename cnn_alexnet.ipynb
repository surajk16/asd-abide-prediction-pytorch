{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'alexnet'\n",
      "=> transfer-learning mode + fine-tuning (train only the last FC layer)\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "=> training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:182: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/42]\tTime 0.967 (0.967)\tData 0.380 (0.380)\tLoss 1.0962 (1.0962)\tPrec@1 55.0 (55.0)\n",
      "Epoch: [0][1/42]\tTime 0.665 (0.816)\tData 0.000 (0.190)\tLoss 1.0938 (1.0950)\tPrec@1 70.0 (62.5)\n",
      "Epoch: [0][2/42]\tTime 0.619 (0.751)\tData 0.000 (0.127)\tLoss 1.0952 (1.0951)\tPrec@1 60.0 (61.66666793823242)\n",
      "Epoch: [0][3/42]\tTime 0.577 (0.707)\tData 0.000 (0.095)\tLoss 1.0952 (1.0951)\tPrec@1 50.0 (58.75)\n",
      "Epoch: [0][4/42]\tTime 0.580 (0.682)\tData 0.000 (0.076)\tLoss 1.0932 (1.0947)\tPrec@1 50.0 (57.0)\n",
      "Epoch: [0][5/42]\tTime 0.570 (0.663)\tData 0.000 (0.063)\tLoss 1.0932 (1.0945)\tPrec@1 50.0 (55.83333206176758)\n",
      "Epoch: [0][6/42]\tTime 0.571 (0.650)\tData 0.000 (0.054)\tLoss 1.0886 (1.0936)\tPrec@1 80.0 (59.28571319580078)\n",
      "Epoch: [0][7/42]\tTime 0.571 (0.640)\tData 0.000 (0.048)\tLoss 1.0903 (1.0932)\tPrec@1 50.0 (58.125)\n",
      "Epoch: [0][8/42]\tTime 0.586 (0.634)\tData 0.006 (0.043)\tLoss 1.0886 (1.0927)\tPrec@1 50.0 (57.22222137451172)\n",
      "Epoch: [0][9/42]\tTime 0.575 (0.628)\tData 0.001 (0.039)\tLoss 1.0852 (1.0919)\tPrec@1 85.0 (60.0)\n",
      "Epoch: [0][10/42]\tTime 0.570 (0.623)\tData 0.000 (0.035)\tLoss 1.0841 (1.0912)\tPrec@1 65.0 (60.45454406738281)\n",
      "Epoch: [0][11/42]\tTime 0.578 (0.619)\tData 0.000 (0.032)\tLoss 1.0842 (1.0906)\tPrec@1 50.0 (59.58333206176758)\n",
      "Epoch: [0][12/42]\tTime 0.579 (0.616)\tData 0.000 (0.030)\tLoss 1.0812 (1.0899)\tPrec@1 70.0 (60.38461685180664)\n",
      "Epoch: [0][13/42]\tTime 0.573 (0.613)\tData 0.000 (0.028)\tLoss 1.0796 (1.0892)\tPrec@1 75.0 (61.42856979370117)\n",
      "Epoch: [0][14/42]\tTime 0.572 (0.610)\tData 0.000 (0.026)\tLoss 1.0772 (1.0884)\tPrec@1 65.0 (61.66666793823242)\n",
      "Epoch: [0][15/42]\tTime 0.570 (0.608)\tData 0.000 (0.024)\tLoss 1.0759 (1.0876)\tPrec@1 70.0 (62.1875)\n",
      "Epoch: [0][16/42]\tTime 0.583 (0.606)\tData 0.001 (0.023)\tLoss 1.0737 (1.0868)\tPrec@1 70.0 (62.64706039428711)\n",
      "Epoch: [0][17/42]\tTime 0.576 (0.605)\tData 0.001 (0.022)\tLoss 1.0729 (1.0860)\tPrec@1 55.0 (62.22222137451172)\n",
      "Epoch: [0][18/42]\tTime 0.575 (0.603)\tData 0.002 (0.021)\tLoss 1.0719 (1.0853)\tPrec@1 60.0 (62.105262756347656)\n",
      "Epoch: [0][19/42]\tTime 0.574 (0.602)\tData 0.002 (0.020)\tLoss 1.0695 (1.0845)\tPrec@1 55.0 (61.75)\n",
      "Epoch: [0][20/42]\tTime 0.580 (0.601)\tData 0.002 (0.019)\tLoss 1.0666 (1.0836)\tPrec@1 70.0 (62.14285659790039)\n",
      "Epoch: [0][21/42]\tTime 0.620 (0.601)\tData 0.001 (0.018)\tLoss 1.0663 (1.0828)\tPrec@1 45.0 (61.3636360168457)\n",
      "Epoch: [0][22/42]\tTime 0.586 (0.601)\tData 0.001 (0.017)\tLoss 1.0618 (1.0819)\tPrec@1 80.0 (62.173912048339844)\n",
      "Epoch: [0][23/42]\tTime 0.579 (0.600)\tData 0.002 (0.017)\tLoss 1.0611 (1.0811)\tPrec@1 65.0 (62.29166793823242)\n",
      "Epoch: [0][24/42]\tTime 0.583 (0.599)\tData 0.002 (0.016)\tLoss 1.0596 (1.0802)\tPrec@1 55.0 (62.0)\n",
      "Epoch: [0][25/42]\tTime 0.598 (0.599)\tData 0.006 (0.016)\tLoss 1.0568 (1.0793)\tPrec@1 85.0 (62.88461685180664)\n",
      "Epoch: [0][26/42]\tTime 0.745 (0.605)\tData 0.002 (0.015)\tLoss 1.0556 (1.0784)\tPrec@1 60.0 (62.77777862548828)\n",
      "Epoch: [0][27/42]\tTime 0.561 (0.603)\tData 0.002 (0.015)\tLoss 1.0524 (1.0775)\tPrec@1 80.0 (63.39285659790039)\n",
      "Epoch: [0][28/42]\tTime 0.567 (0.602)\tData 0.001 (0.014)\tLoss 1.0515 (1.0766)\tPrec@1 55.0 (63.10344696044922)\n",
      "Epoch: [0][29/42]\tTime 0.557 (0.600)\tData 0.001 (0.014)\tLoss 1.0504 (1.0757)\tPrec@1 55.0 (62.83333206176758)\n",
      "Epoch: [0][30/42]\tTime 0.559 (0.599)\tData 0.001 (0.013)\tLoss 1.0489 (1.0749)\tPrec@1 70.0 (63.064517974853516)\n",
      "Epoch: [0][31/42]\tTime 0.551 (0.597)\tData 0.001 (0.013)\tLoss 1.0452 (1.0739)\tPrec@1 85.0 (63.75)\n",
      "Epoch: [0][32/42]\tTime 0.559 (0.596)\tData 0.001 (0.013)\tLoss 1.0442 (1.0730)\tPrec@1 60.0 (63.6363639831543)\n",
      "Epoch: [0][33/42]\tTime 0.561 (0.595)\tData 0.001 (0.012)\tLoss 1.0416 (1.0721)\tPrec@1 65.0 (63.67647171020508)\n",
      "Epoch: [0][34/42]\tTime 0.563 (0.594)\tData 0.001 (0.012)\tLoss 1.0403 (1.0712)\tPrec@1 60.0 (63.57143020629883)\n",
      "Epoch: [0][35/42]\tTime 0.561 (0.593)\tData 0.001 (0.012)\tLoss 1.0399 (1.0703)\tPrec@1 60.0 (63.47222137451172)\n",
      "Epoch: [0][36/42]\tTime 0.553 (0.592)\tData 0.001 (0.012)\tLoss 1.0377 (1.0694)\tPrec@1 45.0 (62.97297286987305)\n",
      "Epoch: [0][37/42]\tTime 0.566 (0.592)\tData 0.001 (0.011)\tLoss 1.0337 (1.0685)\tPrec@1 60.0 (62.894737243652344)\n",
      "Epoch: [0][38/42]\tTime 0.555 (0.591)\tData 0.001 (0.011)\tLoss 1.0335 (1.0676)\tPrec@1 50.0 (62.56410217285156)\n",
      "Epoch: [0][39/42]\tTime 0.583 (0.590)\tData 0.001 (0.011)\tLoss 1.0302 (1.0667)\tPrec@1 60.0 (62.5)\n",
      "Epoch: [0][40/42]\tTime 0.556 (0.590)\tData 0.001 (0.011)\tLoss 1.0297 (1.0658)\tPrec@1 45.0 (62.07316970825195)\n",
      "Epoch: [0][41/42]\tTime 0.284 (0.582)\tData 0.001 (0.010)\tLoss 1.0295 (1.0654)\tPrec@1 54.44444274902344 (61.99034881591797)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:220: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:221: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/suraj/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:239: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/11]\tTime 1.107 (1.107)\tLoss 1.0296 (1.0296)\tPrec@1 55.0 (55.0)\n",
      "Test: [1/11]\tTime 0.538 (0.823)\tLoss 1.0298 (1.0297)\tPrec@1 55.0 (55.0)\n",
      "Test: [2/11]\tTime 0.536 (0.727)\tLoss 1.0285 (1.0293)\tPrec@1 65.0 (58.33333206176758)\n",
      "Test: [3/11]\tTime 0.528 (0.677)\tLoss 1.0293 (1.0293)\tPrec@1 60.0 (58.75)\n",
      "Test: [4/11]\tTime 0.533 (0.649)\tLoss 1.0291 (1.0293)\tPrec@1 60.0 (59.0)\n",
      "Test: [5/11]\tTime 0.525 (0.628)\tLoss 1.0284 (1.0291)\tPrec@1 65.0 (60.0)\n",
      "Test: [6/11]\tTime 0.536 (0.615)\tLoss 1.0293 (1.0291)\tPrec@1 60.0 (60.0)\n",
      "Test: [7/11]\tTime 0.537 (0.605)\tLoss 1.0291 (1.0291)\tPrec@1 60.0 (60.0)\n",
      "Test: [8/11]\tTime 0.533 (0.597)\tLoss 1.0294 (1.0292)\tPrec@1 60.0 (60.0)\n",
      "Test: [9/11]\tTime 0.530 (0.590)\tLoss 1.0280 (1.0290)\tPrec@1 70.0 (61.0)\n",
      "Test: [10/11]\tTime 0.175 (0.553)\tLoss 1.0314 (1.0291)\tPrec@1 43.33333206176758 (60.485435485839844)\n",
      " * Prec@1 60.485435485839844\n",
      "Epoch: [1][0/42]\tTime 1.167 (1.167)\tData 0.450 (0.450)\tLoss 1.0230 (1.0230)\tPrec@1 80.0 (80.0)\n",
      "Epoch: [1][1/42]\tTime 0.530 (0.848)\tData 0.006 (0.228)\tLoss 1.0252 (1.0241)\tPrec@1 60.0 (70.0)\n",
      "Epoch: [1][2/42]\tTime 0.569 (0.755)\tData 0.000 (0.152)\tLoss 1.0226 (1.0236)\tPrec@1 65.0 (68.33333206176758)\n",
      "Epoch: [1][3/42]\tTime 0.566 (0.708)\tData 0.000 (0.114)\tLoss 1.0206 (1.0229)\tPrec@1 60.0 (66.25)\n",
      "Epoch: [1][4/42]\tTime 0.573 (0.681)\tData 0.000 (0.091)\tLoss 1.0183 (1.0220)\tPrec@1 65.0 (66.0)\n",
      "Epoch: [1][5/42]\tTime 0.563 (0.661)\tData 0.000 (0.076)\tLoss 1.0165 (1.0210)\tPrec@1 60.0 (65.0)\n",
      "Epoch: [1][6/42]\tTime 0.572 (0.649)\tData 0.000 (0.065)\tLoss 1.0134 (1.0200)\tPrec@1 65.0 (65.0)\n",
      "Epoch: [1][7/42]\tTime 0.582 (0.640)\tData 0.000 (0.057)\tLoss 1.0156 (1.0194)\tPrec@1 40.0 (61.875)\n",
      "Epoch: [1][8/42]\tTime 0.687 (0.645)\tData 0.002 (0.051)\tLoss 1.0091 (1.0183)\tPrec@1 75.0 (63.33333206176758)\n",
      "Epoch: [1][9/42]\tTime 0.637 (0.645)\tData 0.004 (0.046)\tLoss 1.0095 (1.0174)\tPrec@1 65.0 (63.5)\n",
      "Epoch: [1][10/42]\tTime 0.620 (0.642)\tData 0.000 (0.042)\tLoss 1.0100 (1.0167)\tPrec@1 60.0 (63.181819915771484)\n",
      "Epoch: [1][11/42]\tTime 0.632 (0.642)\tData 0.000 (0.039)\tLoss 1.0030 (1.0156)\tPrec@1 75.0 (64.16666793823242)\n",
      "Epoch: [1][12/42]\tTime 0.627 (0.640)\tData 0.000 (0.036)\tLoss 1.0030 (1.0146)\tPrec@1 70.0 (64.61538314819336)\n",
      "Epoch: [1][13/42]\tTime 0.671 (0.643)\tData 0.002 (0.033)\tLoss 1.0019 (1.0137)\tPrec@1 70.0 (65.0)\n",
      "Epoch: [1][14/42]\tTime 0.613 (0.641)\tData 0.002 (0.031)\tLoss 1.0015 (1.0129)\tPrec@1 65.0 (65.0)\n",
      "Epoch: [1][15/42]\tTime 0.588 (0.637)\tData 0.000 (0.029)\tLoss 1.0033 (1.0123)\tPrec@1 45.0 (63.75)\n",
      "Epoch: [1][16/42]\tTime 0.603 (0.635)\tData 0.001 (0.028)\tLoss 1.0012 (1.0116)\tPrec@1 50.0 (62.94117736816406)\n",
      "Epoch: [1][17/42]\tTime 0.710 (0.639)\tData 0.001 (0.026)\tLoss 0.9973 (1.0108)\tPrec@1 60.0 (62.77777862548828)\n",
      "Epoch: [1][18/42]\tTime 0.708 (0.643)\tData 0.002 (0.025)\tLoss 0.9943 (1.0100)\tPrec@1 55.0 (62.3684196472168)\n",
      "Epoch: [1][19/42]\tTime 0.624 (0.642)\tData 0.002 (0.024)\tLoss 0.9941 (1.0092)\tPrec@1 60.0 (62.25)\n",
      "Epoch: [1][20/42]\tTime 0.689 (0.644)\tData 0.002 (0.023)\tLoss 0.9926 (1.0084)\tPrec@1 65.0 (62.380950927734375)\n",
      "Epoch: [1][21/42]\tTime 0.575 (0.641)\tData 0.002 (0.022)\tLoss 0.9899 (1.0075)\tPrec@1 60.0 (62.272727966308594)\n",
      "Epoch: [1][22/42]\tTime 0.582 (0.639)\tData 0.001 (0.021)\tLoss 0.9895 (1.0068)\tPrec@1 65.0 (62.39130401611328)\n",
      "Epoch: [1][23/42]\tTime 0.578 (0.636)\tData 0.001 (0.020)\tLoss 0.9912 (1.0061)\tPrec@1 50.0 (61.875)\n",
      "Epoch: [1][24/42]\tTime 0.573 (0.634)\tData 0.002 (0.019)\tLoss 0.9900 (1.0055)\tPrec@1 50.0 (61.400001525878906)\n",
      "Epoch: [1][25/42]\tTime 0.692 (0.636)\tData 0.002 (0.019)\tLoss 0.9832 (1.0046)\tPrec@1 80.0 (62.11538314819336)\n",
      "Epoch: [1][26/42]\tTime 0.697 (0.638)\tData 0.001 (0.018)\tLoss 0.9823 (1.0038)\tPrec@1 70.0 (62.407405853271484)\n",
      "Epoch: [1][27/42]\tTime 0.672 (0.639)\tData 0.003 (0.017)\tLoss 0.9813 (1.0030)\tPrec@1 70.0 (62.67856979370117)\n",
      "Epoch: [1][28/42]\tTime 0.654 (0.640)\tData 0.001 (0.017)\tLoss 0.9805 (1.0022)\tPrec@1 60.0 (62.58620834350586)\n",
      "Epoch: [1][29/42]\tTime 0.644 (0.640)\tData 0.001 (0.016)\tLoss 0.9820 (1.0015)\tPrec@1 50.0 (62.16666793823242)\n",
      "Epoch: [1][30/42]\tTime 0.660 (0.641)\tData 0.002 (0.016)\tLoss 0.9781 (1.0008)\tPrec@1 60.0 (62.09677505493164)\n",
      "Epoch: [1][31/42]\tTime 0.661 (0.641)\tData 0.002 (0.015)\tLoss 0.9779 (1.0001)\tPrec@1 55.0 (61.875)\n",
      "Epoch: [1][32/42]\tTime 0.688 (0.643)\tData 0.001 (0.015)\tLoss 0.9708 (0.9992)\tPrec@1 80.0 (62.42424392700195)\n",
      "Epoch: [1][33/42]\tTime 0.671 (0.644)\tData 0.002 (0.015)\tLoss 0.9743 (0.9984)\tPrec@1 55.0 (62.20588302612305)\n",
      "Epoch: [1][34/42]\tTime 0.567 (0.641)\tData 0.002 (0.014)\tLoss 0.9707 (0.9977)\tPrec@1 65.0 (62.28571319580078)\n",
      "Epoch: [1][35/42]\tTime 0.551 (0.639)\tData 0.001 (0.014)\tLoss 0.9702 (0.9969)\tPrec@1 70.0 (62.5)\n",
      "Epoch: [1][36/42]\tTime 0.564 (0.637)\tData 0.002 (0.014)\tLoss 0.9707 (0.9962)\tPrec@1 55.0 (62.297298431396484)\n",
      "Epoch: [1][37/42]\tTime 0.552 (0.635)\tData 0.001 (0.013)\tLoss 0.9660 (0.9954)\tPrec@1 65.0 (62.3684196472168)\n",
      "Epoch: [1][38/42]\tTime 0.689 (0.636)\tData 0.001 (0.013)\tLoss 0.9712 (0.9948)\tPrec@1 40.0 (61.79487228393555)\n",
      "Epoch: [1][39/42]\tTime 0.598 (0.635)\tData 0.004 (0.013)\tLoss 0.9662 (0.9940)\tPrec@1 60.0 (61.75)\n",
      "Epoch: [1][40/42]\tTime 0.553 (0.633)\tData 0.001 (0.012)\tLoss 0.9665 (0.9934)\tPrec@1 55.0 (61.585365295410156)\n",
      "Epoch: [1][41/42]\tTime 0.270 (0.624)\tData 0.001 (0.012)\tLoss 0.9640 (0.9931)\tPrec@1 65.5555534362793 (61.62846755981445)\n",
      "Test: [0/11]\tTime 1.066 (1.066)\tLoss 0.9693 (0.9693)\tPrec@1 55.0 (55.0)\n",
      "Test: [1/11]\tTime 0.528 (0.797)\tLoss 0.9705 (0.9699)\tPrec@1 50.0 (52.5)\n",
      "Test: [2/11]\tTime 0.538 (0.711)\tLoss 0.9671 (0.9689)\tPrec@1 65.0 (56.66666793823242)\n",
      "Test: [3/11]\tTime 0.530 (0.666)\tLoss 0.9672 (0.9685)\tPrec@1 65.0 (58.75)\n",
      "Test: [4/11]\tTime 0.535 (0.639)\tLoss 0.9727 (0.9693)\tPrec@1 40.0 (55.0)\n",
      "Test: [5/11]\tTime 0.529 (0.621)\tLoss 0.9649 (0.9686)\tPrec@1 75.0 (58.33333206176758)\n",
      "Test: [6/11]\tTime 0.535 (0.609)\tLoss 0.9715 (0.9690)\tPrec@1 45.0 (56.42856979370117)\n",
      "Test: [7/11]\tTime 0.531 (0.599)\tLoss 0.9661 (0.9686)\tPrec@1 70.0 (58.125)\n",
      "Test: [8/11]\tTime 0.535 (0.592)\tLoss 0.9681 (0.9686)\tPrec@1 60.0 (58.33333206176758)\n",
      "Test: [9/11]\tTime 0.530 (0.586)\tLoss 0.9636 (0.9681)\tPrec@1 80.0 (60.5)\n",
      "Test: [10/11]\tTime 0.176 (0.548)\tLoss 0.9684 (0.9681)\tPrec@1 60.0 (60.485435485839844)\n",
      " * Prec@1 60.485435485839844\n",
      "Epoch: [2][0/42]\tTime 0.936 (0.936)\tData 0.245 (0.245)\tLoss 0.9639 (0.9639)\tPrec@1 55.0 (55.0)\n",
      "Epoch: [2][1/42]\tTime 0.559 (0.747)\tData 0.005 (0.125)\tLoss 0.9610 (0.9624)\tPrec@1 55.0 (55.0)\n",
      "Epoch: [2][2/42]\tTime 0.578 (0.691)\tData 0.005 (0.085)\tLoss 0.9568 (0.9605)\tPrec@1 70.0 (60.0)\n",
      "Epoch: [2][3/42]\tTime 0.574 (0.662)\tData 0.000 (0.064)\tLoss 0.9555 (0.9593)\tPrec@1 80.0 (65.0)\n",
      "Epoch: [2][4/42]\tTime 0.570 (0.643)\tData 0.000 (0.051)\tLoss 0.9556 (0.9585)\tPrec@1 70.0 (66.0)\n",
      "Epoch: [2][5/42]\tTime 0.574 (0.632)\tData 0.000 (0.043)\tLoss 0.9576 (0.9584)\tPrec@1 55.0 (64.16666793823242)\n",
      "Epoch: [2][6/42]\tTime 0.569 (0.623)\tData 0.000 (0.036)\tLoss 0.9515 (0.9574)\tPrec@1 75.0 (65.71428680419922)\n",
      "Epoch: [2][7/42]\tTime 0.579 (0.617)\tData 0.000 (0.032)\tLoss 0.9510 (0.9566)\tPrec@1 70.0 (66.25)\n",
      "Epoch: [2][8/42]\tTime 0.587 (0.614)\tData 0.000 (0.028)\tLoss 0.9490 (0.9557)\tPrec@1 75.0 (67.22222137451172)\n",
      "Epoch: [2][9/42]\tTime 0.573 (0.610)\tData 0.004 (0.026)\tLoss 0.9539 (0.9556)\tPrec@1 55.0 (66.0)\n",
      "Epoch: [2][10/42]\tTime 0.602 (0.609)\tData 0.003 (0.024)\tLoss 0.9508 (0.9551)\tPrec@1 55.0 (65.0)\n",
      "Epoch: [2][11/42]\tTime 0.582 (0.607)\tData 0.000 (0.022)\tLoss 0.9509 (0.9548)\tPrec@1 55.0 (64.16666793823242)\n",
      "Epoch: [2][12/42]\tTime 0.574 (0.604)\tData 0.000 (0.020)\tLoss 0.9457 (0.9541)\tPrec@1 65.0 (64.23077011108398)\n",
      "Epoch: [2][13/42]\tTime 0.569 (0.602)\tData 0.000 (0.019)\tLoss 0.9519 (0.9539)\tPrec@1 40.0 (62.5)\n",
      "Epoch: [2][14/42]\tTime 0.575 (0.600)\tData 0.000 (0.018)\tLoss 0.9393 (0.9529)\tPrec@1 80.0 (63.66666793823242)\n",
      "Epoch: [2][15/42]\tTime 0.584 (0.599)\tData 0.000 (0.017)\tLoss 0.9482 (0.9526)\tPrec@1 45.0 (62.5)\n",
      "Epoch: [2][16/42]\tTime 0.570 (0.597)\tData 0.000 (0.016)\tLoss 0.9428 (0.9521)\tPrec@1 55.0 (62.05882263183594)\n",
      "Epoch: [2][17/42]\tTime 0.578 (0.596)\tData 0.002 (0.015)\tLoss 0.9425 (0.9515)\tPrec@1 65.0 (62.22222137451172)\n",
      "Epoch: [2][18/42]\tTime 0.577 (0.595)\tData 0.001 (0.014)\tLoss 0.9427 (0.9511)\tPrec@1 55.0 (61.842105865478516)\n",
      "Epoch: [2][19/42]\tTime 0.577 (0.594)\tData 0.002 (0.013)\tLoss 0.9349 (0.9503)\tPrec@1 75.0 (62.5)\n",
      "Epoch: [2][20/42]\tTime 0.599 (0.594)\tData 0.002 (0.013)\tLoss 0.9314 (0.9494)\tPrec@1 85.0 (63.57143020629883)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][21/42]\tTime 0.576 (0.594)\tData 0.001 (0.012)\tLoss 0.9377 (0.9488)\tPrec@1 60.0 (63.40909194946289)\n",
      "Epoch: [2][22/42]\tTime 0.574 (0.593)\tData 0.002 (0.012)\tLoss 0.9349 (0.9482)\tPrec@1 65.0 (63.4782600402832)\n",
      "Epoch: [2][23/42]\tTime 0.568 (0.592)\tData 0.001 (0.011)\tLoss 0.9380 (0.9478)\tPrec@1 55.0 (63.125)\n",
      "Epoch: [2][24/42]\tTime 0.582 (0.591)\tData 0.001 (0.011)\tLoss 0.9348 (0.9473)\tPrec@1 60.0 (63.0)\n",
      "Epoch: [2][25/42]\tTime 0.562 (0.590)\tData 0.002 (0.011)\tLoss 0.9314 (0.9467)\tPrec@1 60.0 (62.88461685180664)\n",
      "Epoch: [2][26/42]\tTime 0.554 (0.589)\tData 0.001 (0.010)\tLoss 0.9380 (0.9463)\tPrec@1 45.0 (62.22222137451172)\n",
      "Epoch: [2][27/42]\tTime 0.549 (0.587)\tData 0.002 (0.010)\tLoss 0.9301 (0.9458)\tPrec@1 65.0 (62.32143020629883)\n",
      "Epoch: [2][28/42]\tTime 0.554 (0.586)\tData 0.002 (0.010)\tLoss 0.9323 (0.9453)\tPrec@1 60.0 (62.24137878417969)\n",
      "Epoch: [2][29/42]\tTime 0.557 (0.585)\tData 0.001 (0.009)\tLoss 0.9281 (0.9447)\tPrec@1 65.0 (62.33333206176758)\n",
      "Epoch: [2][30/42]\tTime 0.547 (0.584)\tData 0.001 (0.009)\tLoss 0.9242 (0.9441)\tPrec@1 70.0 (62.58064651489258)\n",
      "Epoch: [2][31/42]\tTime 0.553 (0.583)\tData 0.002 (0.009)\tLoss 0.9291 (0.9436)\tPrec@1 55.0 (62.34375)\n",
      "Epoch: [2][32/42]\tTime 0.544 (0.582)\tData 0.001 (0.009)\tLoss 0.9290 (0.9432)\tPrec@1 55.0 (62.121212005615234)\n",
      "Epoch: [2][33/42]\tTime 0.555 (0.581)\tData 0.001 (0.009)\tLoss 0.9364 (0.9430)\tPrec@1 30.0 (61.17647171020508)\n",
      "Epoch: [2][34/42]\tTime 0.551 (0.580)\tData 0.002 (0.008)\tLoss 0.9212 (0.9423)\tPrec@1 70.0 (61.42856979370117)\n",
      "Epoch: [2][35/42]\tTime 0.551 (0.579)\tData 0.001 (0.008)\tLoss 0.9167 (0.9416)\tPrec@1 80.0 (61.94444274902344)\n",
      "Epoch: [2][36/42]\tTime 0.553 (0.579)\tData 0.001 (0.008)\tLoss 0.9194 (0.9410)\tPrec@1 70.0 (62.16216278076172)\n",
      "Epoch: [2][37/42]\tTime 0.555 (0.578)\tData 0.001 (0.008)\tLoss 0.9273 (0.9407)\tPrec@1 45.0 (61.71052551269531)\n",
      "Epoch: [2][38/42]\tTime 0.558 (0.578)\tData 0.001 (0.008)\tLoss 0.9197 (0.9401)\tPrec@1 60.0 (61.66666793823242)\n",
      "Epoch: [2][39/42]\tTime 0.548 (0.577)\tData 0.002 (0.007)\tLoss 0.9203 (0.9396)\tPrec@1 60.0 (61.625)\n",
      "Epoch: [2][40/42]\tTime 0.556 (0.576)\tData 0.001 (0.007)\tLoss 0.9230 (0.9392)\tPrec@1 50.0 (61.34146499633789)\n",
      "Epoch: [2][41/42]\tTime 0.275 (0.569)\tData 0.001 (0.007)\tLoss 0.9225 (0.9390)\tPrec@1 54.44444274902344 (61.26658630371094)\n",
      "Test: [0/11]\tTime 1.078 (1.078)\tLoss 0.9297 (0.9297)\tPrec@1 45.0 (45.0)\n",
      "Test: [1/11]\tTime 0.534 (0.806)\tLoss 0.9264 (0.9280)\tPrec@1 55.0 (50.0)\n",
      "Test: [2/11]\tTime 0.540 (0.718)\tLoss 0.9178 (0.9246)\tPrec@1 80.0 (60.0)\n",
      "Test: [3/11]\tTime 0.533 (0.671)\tLoss 0.9279 (0.9254)\tPrec@1 50.0 (57.5)\n",
      "Test: [4/11]\tTime 0.528 (0.643)\tLoss 0.9230 (0.9250)\tPrec@1 65.0 (59.0)\n",
      "Test: [5/11]\tTime 0.537 (0.625)\tLoss 0.9196 (0.9241)\tPrec@1 75.0 (61.66666793823242)\n",
      "Test: [6/11]\tTime 0.529 (0.611)\tLoss 0.9298 (0.9249)\tPrec@1 45.0 (59.28571319580078)\n",
      "Test: [7/11]\tTime 0.533 (0.602)\tLoss 0.9261 (0.9250)\tPrec@1 55.0 (58.75)\n",
      "Test: [8/11]\tTime 0.530 (0.594)\tLoss 0.9213 (0.9246)\tPrec@1 70.0 (60.0)\n",
      "Test: [9/11]\tTime 0.535 (0.588)\tLoss 0.9247 (0.9246)\tPrec@1 60.0 (60.0)\n",
      "Test: [10/11]\tTime 0.173 (0.550)\tLoss 0.9192 (0.9245)\tPrec@1 76.66666412353516 (60.485435485839844)\n",
      " * Prec@1 60.485435485839844\n",
      "Epoch: [3][0/42]\tTime 0.939 (0.939)\tData 0.250 (0.250)\tLoss 0.9112 (0.9112)\tPrec@1 80.0 (80.0)\n",
      "Epoch: [3][1/42]\tTime 0.558 (0.748)\tData 0.001 (0.126)\tLoss 0.9130 (0.9121)\tPrec@1 70.0 (75.0)\n",
      "Epoch: [3][2/42]\tTime 0.584 (0.694)\tData 0.005 (0.085)\tLoss 0.9128 (0.9123)\tPrec@1 70.0 (73.33333206176758)\n",
      "Epoch: [3][3/42]\tTime 0.572 (0.663)\tData 0.000 (0.064)\tLoss 0.9148 (0.9129)\tPrec@1 65.0 (71.25)\n",
      "Epoch: [3][4/42]\tTime 0.574 (0.645)\tData 0.003 (0.052)\tLoss 0.9185 (0.9140)\tPrec@1 45.0 (66.0)\n",
      "Epoch: [3][5/42]\tTime 0.576 (0.634)\tData 0.000 (0.043)\tLoss 0.9097 (0.9133)\tPrec@1 65.0 (65.83333206176758)\n",
      "Epoch: [3][6/42]\tTime 0.575 (0.625)\tData 0.000 (0.037)\tLoss 0.9027 (0.9118)\tPrec@1 85.0 (68.57143020629883)\n",
      "Epoch: [3][7/42]\tTime 0.568 (0.618)\tData 0.000 (0.033)\tLoss 0.9088 (0.9114)\tPrec@1 65.0 (68.125)\n",
      "Epoch: [3][8/42]\tTime 0.576 (0.614)\tData 0.000 (0.029)\tLoss 0.9146 (0.9118)\tPrec@1 50.0 (66.11111068725586)\n",
      "Epoch: [3][9/42]\tTime 0.576 (0.610)\tData 0.000 (0.026)\tLoss 0.9077 (0.9114)\tPrec@1 60.0 (65.5)\n",
      "Epoch: [3][10/42]\tTime 0.574 (0.606)\tData 0.005 (0.024)\tLoss 0.9056 (0.9108)\tPrec@1 65.0 (65.45454406738281)\n",
      "Epoch: [3][11/42]\tTime 0.576 (0.604)\tData 0.000 (0.022)\tLoss 0.9051 (0.9104)\tPrec@1 65.0 (65.41666793823242)\n",
      "Epoch: [3][12/42]\tTime 0.571 (0.601)\tData 0.002 (0.021)\tLoss 0.9064 (0.9101)\tPrec@1 60.0 (65.0)\n",
      "Epoch: [3][13/42]\tTime 0.573 (0.599)\tData 0.000 (0.019)\tLoss 0.9111 (0.9101)\tPrec@1 50.0 (63.92856979370117)\n",
      "Epoch: [3][14/42]\tTime 0.569 (0.597)\tData 0.000 (0.018)\tLoss 0.9140 (0.9104)\tPrec@1 35.0 (62.0)\n",
      "Epoch: [3][15/42]\tTime 0.577 (0.596)\tData 0.002 (0.017)\tLoss 0.9080 (0.9103)\tPrec@1 55.0 (61.5625)\n",
      "Epoch: [3][16/42]\tTime 0.576 (0.595)\tData 0.000 (0.016)\tLoss 0.9014 (0.9097)\tPrec@1 70.0 (62.05882263183594)\n",
      "Epoch: [3][17/42]\tTime 0.572 (0.594)\tData 0.002 (0.015)\tLoss 0.9008 (0.9092)\tPrec@1 65.0 (62.22222137451172)\n",
      "Epoch: [3][18/42]\tTime 0.575 (0.593)\tData 0.002 (0.014)\tLoss 0.9065 (0.9091)\tPrec@1 55.0 (61.842105865478516)\n",
      "Epoch: [3][19/42]\tTime 0.571 (0.592)\tData 0.001 (0.014)\tLoss 0.8994 (0.9086)\tPrec@1 65.0 (62.0)\n",
      "Epoch: [3][20/42]\tTime 0.576 (0.591)\tData 0.001 (0.013)\tLoss 0.9102 (0.9087)\tPrec@1 35.0 (60.71428680419922)\n",
      "Epoch: [3][21/42]\tTime 0.657 (0.594)\tData 0.002 (0.013)\tLoss 0.8962 (0.9081)\tPrec@1 70.0 (61.1363639831543)\n",
      "Epoch: [3][22/42]\tTime 0.570 (0.593)\tData 0.002 (0.012)\tLoss 0.8965 (0.9076)\tPrec@1 70.0 (61.5217399597168)\n",
      "Epoch: [3][23/42]\tTime 0.578 (0.592)\tData 0.001 (0.012)\tLoss 0.8944 (0.9071)\tPrec@1 70.0 (61.875)\n",
      "Epoch: [3][24/42]\tTime 0.569 (0.591)\tData 0.001 (0.011)\tLoss 0.8870 (0.9063)\tPrec@1 90.0 (63.0)\n",
      "Epoch: [3][25/42]\tTime 0.568 (0.590)\tData 0.001 (0.011)\tLoss 0.8989 (0.9060)\tPrec@1 55.0 (62.69230651855469)\n",
      "Epoch: [3][26/42]\tTime 0.559 (0.589)\tData 0.001 (0.011)\tLoss 0.9017 (0.9058)\tPrec@1 45.0 (62.03703689575195)\n",
      "Epoch: [3][27/42]\tTime 0.559 (0.588)\tData 0.001 (0.010)\tLoss 0.8986 (0.9056)\tPrec@1 55.0 (61.78571319580078)\n",
      "Epoch: [3][28/42]\tTime 0.561 (0.587)\tData 0.001 (0.010)\tLoss 0.8991 (0.9053)\tPrec@1 50.0 (61.379310607910156)\n",
      "Epoch: [3][29/42]\tTime 0.558 (0.586)\tData 0.002 (0.010)\tLoss 0.8935 (0.9049)\tPrec@1 60.0 (61.33333206176758)\n",
      "Epoch: [3][30/42]\tTime 0.558 (0.585)\tData 0.002 (0.009)\tLoss 0.8993 (0.9048)\tPrec@1 45.0 (60.80644989013672)\n",
      "Epoch: [3][31/42]\tTime 0.555 (0.584)\tData 0.001 (0.009)\tLoss 0.8920 (0.9044)\tPrec@1 60.0 (60.78125)\n",
      "Epoch: [3][32/42]\tTime 0.558 (0.584)\tData 0.001 (0.009)\tLoss 0.8942 (0.9041)\tPrec@1 50.0 (60.45454406738281)\n",
      "Epoch: [3][33/42]\tTime 0.553 (0.583)\tData 0.001 (0.009)\tLoss 0.8821 (0.9034)\tPrec@1 80.0 (61.02941131591797)\n",
      "Epoch: [3][34/42]\tTime 0.561 (0.582)\tData 0.001 (0.008)\tLoss 0.8864 (0.9029)\tPrec@1 70.0 (61.28571319580078)\n",
      "Epoch: [3][35/42]\tTime 0.552 (0.581)\tData 0.001 (0.008)\tLoss 0.8916 (0.9026)\tPrec@1 60.0 (61.25)\n",
      "Epoch: [3][36/42]\tTime 0.551 (0.580)\tData 0.001 (0.008)\tLoss 0.8896 (0.9023)\tPrec@1 60.0 (61.216217041015625)\n",
      "Epoch: [3][37/42]\tTime 0.557 (0.580)\tData 0.001 (0.008)\tLoss 0.8876 (0.9019)\tPrec@1 60.0 (61.18421173095703)\n",
      "Epoch: [3][38/42]\tTime 0.557 (0.579)\tData 0.002 (0.008)\tLoss 0.8865 (0.9015)\tPrec@1 65.0 (61.28205108642578)\n",
      "Epoch: [3][39/42]\tTime 0.556 (0.579)\tData 0.001 (0.008)\tLoss 0.8842 (0.9010)\tPrec@1 70.0 (61.5)\n",
      "Epoch: [3][40/42]\tTime 0.550 (0.578)\tData 0.001 (0.007)\tLoss 0.8851 (0.9007)\tPrec@1 65.0 (61.585365295410156)\n",
      "Epoch: [3][41/42]\tTime 0.276 (0.571)\tData 0.001 (0.007)\tLoss 0.8903 (0.9005)\tPrec@1 43.33333206176758 (61.38721466064453)\n",
      "Test: [0/11]\tTime 1.038 (1.038)\tLoss 0.8888 (0.8888)\tPrec@1 70.0 (70.0)\n",
      "Test: [1/11]\tTime 0.532 (0.785)\tLoss 0.8854 (0.8871)\tPrec@1 80.0 (75.0)\n",
      "Test: [2/11]\tTime 0.540 (0.703)\tLoss 0.8939 (0.8894)\tPrec@1 55.0 (68.33333206176758)\n",
      "Test: [3/11]\tTime 0.528 (0.660)\tLoss 0.8960 (0.8910)\tPrec@1 50.0 (63.75)\n",
      "Test: [4/11]\tTime 0.538 (0.635)\tLoss 0.8888 (0.8906)\tPrec@1 70.0 (65.0)\n",
      "Test: [5/11]\tTime 0.534 (0.618)\tLoss 0.8957 (0.8914)\tPrec@1 50.0 (62.5)\n",
      "Test: [6/11]\tTime 0.535 (0.606)\tLoss 0.8943 (0.8918)\tPrec@1 55.0 (61.42856979370117)\n",
      "Test: [7/11]\tTime 0.533 (0.597)\tLoss 0.8889 (0.8915)\tPrec@1 70.0 (62.5)\n",
      "Test: [8/11]\tTime 0.540 (0.591)\tLoss 0.8922 (0.8916)\tPrec@1 60.0 (62.22222137451172)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [9/11]\tTime 0.530 (0.585)\tLoss 0.8959 (0.8920)\tPrec@1 50.0 (61.0)\n",
      "Test: [10/11]\tTime 0.175 (0.547)\tLoss 0.8984 (0.8922)\tPrec@1 43.33333206176758 (60.485435485839844)\n",
      " * Prec@1 60.485435485839844\n",
      "Epoch: [4][0/42]\tTime 0.966 (0.966)\tData 0.275 (0.275)\tLoss 0.8851 (0.8851)\tPrec@1 60.0 (60.0)\n",
      "Epoch: [4][1/42]\tTime 0.546 (0.756)\tData 0.001 (0.138)\tLoss 0.8866 (0.8858)\tPrec@1 55.0 (57.5)\n",
      "Epoch: [4][2/42]\tTime 0.572 (0.695)\tData 0.004 (0.093)\tLoss 0.8874 (0.8864)\tPrec@1 50.0 (55.0)\n",
      "Epoch: [4][3/42]\tTime 0.576 (0.665)\tData 0.000 (0.070)\tLoss 0.8771 (0.8841)\tPrec@1 75.0 (60.0)\n",
      "Epoch: [4][4/42]\tTime 0.566 (0.645)\tData 0.001 (0.056)\tLoss 0.8820 (0.8836)\tPrec@1 60.0 (60.0)\n",
      "Epoch: [4][5/42]\tTime 0.578 (0.634)\tData 0.002 (0.047)\tLoss 0.8809 (0.8832)\tPrec@1 60.0 (60.0)\n",
      "Epoch: [4][6/42]\tTime 0.636 (0.634)\tData 0.000 (0.040)\tLoss 0.8745 (0.8820)\tPrec@1 75.0 (62.14285659790039)\n",
      "Epoch: [4][7/42]\tTime 0.589 (0.629)\tData 0.002 (0.036)\tLoss 0.8806 (0.8818)\tPrec@1 60.0 (61.875)\n",
      "Epoch: [4][8/42]\tTime 0.582 (0.623)\tData 0.001 (0.032)\tLoss 0.8786 (0.8814)\tPrec@1 60.0 (61.66666793823242)\n",
      "Epoch: [4][9/42]\tTime 0.576 (0.619)\tData 0.000 (0.029)\tLoss 0.8783 (0.8811)\tPrec@1 60.0 (61.5)\n",
      "Epoch: [4][10/42]\tTime 0.582 (0.615)\tData 0.002 (0.026)\tLoss 0.8734 (0.8804)\tPrec@1 70.0 (62.272727966308594)\n",
      "Epoch: [4][11/42]\tTime 0.576 (0.612)\tData 0.002 (0.024)\tLoss 0.8825 (0.8806)\tPrec@1 45.0 (60.83333206176758)\n",
      "Epoch: [4][12/42]\tTime 0.570 (0.609)\tData 0.004 (0.023)\tLoss 0.8766 (0.8803)\tPrec@1 60.0 (60.769229888916016)\n",
      "Epoch: [4][13/42]\tTime 0.574 (0.606)\tData 0.000 (0.021)\tLoss 0.8763 (0.8800)\tPrec@1 60.0 (60.71428680419922)\n",
      "Epoch: [4][14/42]\tTime 0.568 (0.604)\tData 0.000 (0.020)\tLoss 0.8686 (0.8792)\tPrec@1 80.0 (62.0)\n",
      "Epoch: [4][15/42]\tTime 0.576 (0.602)\tData 0.000 (0.019)\tLoss 0.8713 (0.8787)\tPrec@1 65.0 (62.1875)\n",
      "Epoch: [4][16/42]\tTime 0.571 (0.600)\tData 0.000 (0.017)\tLoss 0.8721 (0.8784)\tPrec@1 65.0 (62.35293960571289)\n",
      "Epoch: [4][17/42]\tTime 0.572 (0.599)\tData 0.002 (0.017)\tLoss 0.8760 (0.8782)\tPrec@1 55.0 (61.94444274902344)\n",
      "Epoch: [4][18/42]\tTime 0.583 (0.598)\tData 0.001 (0.016)\tLoss 0.8692 (0.8777)\tPrec@1 70.0 (62.3684196472168)\n",
      "Epoch: [4][19/42]\tTime 0.571 (0.596)\tData 0.002 (0.015)\tLoss 0.8667 (0.8772)\tPrec@1 75.0 (63.0)\n",
      "Epoch: [4][20/42]\tTime 0.573 (0.595)\tData 0.002 (0.014)\tLoss 0.8771 (0.8772)\tPrec@1 50.0 (62.380950927734375)\n",
      "Epoch: [4][21/42]\tTime 0.572 (0.594)\tData 0.002 (0.014)\tLoss 0.8806 (0.8773)\tPrec@1 35.0 (61.1363639831543)\n",
      "Epoch: [4][22/42]\tTime 0.576 (0.594)\tData 0.002 (0.013)\tLoss 0.8674 (0.8769)\tPrec@1 65.0 (61.30434799194336)\n",
      "Epoch: [4][23/42]\tTime 0.569 (0.593)\tData 0.002 (0.013)\tLoss 0.8660 (0.8765)\tPrec@1 70.0 (61.66666793823242)\n",
      "Epoch: [4][24/42]\tTime 0.573 (0.592)\tData 0.002 (0.012)\tLoss 0.8706 (0.8762)\tPrec@1 60.0 (61.599998474121094)\n",
      "Epoch: [4][25/42]\tTime 0.562 (0.591)\tData 0.002 (0.012)\tLoss 0.8674 (0.8759)\tPrec@1 65.0 (61.730770111083984)\n",
      "Epoch: [4][26/42]\tTime 0.550 (0.589)\tData 0.001 (0.012)\tLoss 0.8681 (0.8756)\tPrec@1 60.0 (61.66666793823242)\n",
      "Epoch: [4][27/42]\tTime 0.557 (0.588)\tData 0.001 (0.011)\tLoss 0.8728 (0.8755)\tPrec@1 50.0 (61.25)\n",
      "Epoch: [4][28/42]\tTime 0.547 (0.587)\tData 0.001 (0.011)\tLoss 0.8700 (0.8753)\tPrec@1 55.0 (61.034481048583984)\n",
      "Epoch: [4][29/42]\tTime 0.553 (0.585)\tData 0.001 (0.011)\tLoss 0.8635 (0.8749)\tPrec@1 65.0 (61.16666793823242)\n",
      "Epoch: [4][30/42]\tTime 0.551 (0.584)\tData 0.001 (0.010)\tLoss 0.8731 (0.8749)\tPrec@1 45.0 (60.64516067504883)\n",
      "Epoch: [4][31/42]\tTime 0.563 (0.584)\tData 0.001 (0.010)\tLoss 0.8648 (0.8745)\tPrec@1 65.0 (60.78125)\n",
      "Epoch: [4][32/42]\tTime 0.550 (0.583)\tData 0.001 (0.010)\tLoss 0.8584 (0.8741)\tPrec@1 75.0 (61.212120056152344)\n",
      "Epoch: [4][33/42]\tTime 0.554 (0.582)\tData 0.001 (0.009)\tLoss 0.8612 (0.8737)\tPrec@1 70.0 (61.47058868408203)\n",
      "Epoch: [4][34/42]\tTime 0.550 (0.581)\tData 0.001 (0.009)\tLoss 0.8646 (0.8734)\tPrec@1 55.0 (61.28571319580078)\n",
      "Epoch: [4][35/42]\tTime 0.557 (0.580)\tData 0.002 (0.009)\tLoss 0.8603 (0.8731)\tPrec@1 65.0 (61.38888931274414)\n",
      "Epoch: [4][36/42]\tTime 0.554 (0.580)\tData 0.001 (0.009)\tLoss 0.8705 (0.8730)\tPrec@1 45.0 (60.945945739746094)\n",
      "Epoch: [4][37/42]\tTime 0.553 (0.579)\tData 0.001 (0.009)\tLoss 0.8584 (0.8726)\tPrec@1 70.0 (61.18421173095703)\n",
      "Epoch: [4][38/42]\tTime 0.560 (0.578)\tData 0.001 (0.008)\tLoss 0.8614 (0.8723)\tPrec@1 65.0 (61.28205108642578)\n",
      "Epoch: [4][39/42]\tTime 0.550 (0.578)\tData 0.001 (0.008)\tLoss 0.8583 (0.8720)\tPrec@1 65.0 (61.375)\n",
      "Epoch: [4][40/42]\tTime 0.554 (0.577)\tData 0.001 (0.008)\tLoss 0.8608 (0.8717)\tPrec@1 60.0 (61.34146499633789)\n",
      "Epoch: [4][41/42]\tTime 0.273 (0.570)\tData 0.001 (0.008)\tLoss 0.8550 (0.8715)\tPrec@1 65.5555534362793 (61.38721466064453)\n",
      "Test: [0/11]\tTime 1.017 (1.017)\tLoss 0.8679 (0.8679)\tPrec@1 60.0 (60.0)\n",
      "Test: [1/11]\tTime 0.536 (0.777)\tLoss 0.8676 (0.8677)\tPrec@1 60.0 (60.0)\n",
      "Test: [2/11]\tTime 0.532 (0.695)\tLoss 0.8619 (0.8658)\tPrec@1 75.0 (65.0)\n",
      "Test: [3/11]\tTime 0.537 (0.655)\tLoss 0.8715 (0.8672)\tPrec@1 50.0 (61.25)\n",
      "Test: [4/11]\tTime 0.530 (0.630)\tLoss 0.8697 (0.8677)\tPrec@1 55.0 (60.0)\n",
      "Test: [5/11]\tTime 0.536 (0.615)\tLoss 0.8656 (0.8674)\tPrec@1 65.0 (60.83333206176758)\n",
      "Test: [6/11]\tTime 0.529 (0.602)\tLoss 0.8695 (0.8677)\tPrec@1 55.0 (60.0)\n",
      "Test: [7/11]\tTime 0.537 (0.594)\tLoss 0.8658 (0.8674)\tPrec@1 65.0 (60.625)\n",
      "Test: [8/11]\tTime 0.532 (0.587)\tLoss 0.8658 (0.8673)\tPrec@1 65.0 (61.11111068725586)\n",
      "Test: [9/11]\tTime 0.551 (0.584)\tLoss 0.8714 (0.8677)\tPrec@1 50.0 (60.0)\n",
      "Test: [10/11]\tTime 0.173 (0.546)\tLoss 0.8611 (0.8675)\tPrec@1 76.66666412353516 (60.485435485839844)\n",
      " * Prec@1 60.485435485839844\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset  # For custom datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "validation_split = .2\n",
    "shuffle_dataset = False\n",
    "random_seed= 42\n",
    "num_epochs = 5\n",
    "num_classes = 2\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001\n",
    "momentum = 0.9\n",
    "print_freq = 1\n",
    "best_prec1 = 0\n",
    "workers = 8\n",
    "pretrained = False\n",
    "fine_tune = True\n",
    "arch = 'alexnet'\n",
    "classes = [1,2]\n",
    "\n",
    "dir = '/home/suraj/asd-abide-prediction-pytorch/'\n",
    "\n",
    "#dataset\n",
    "\n",
    "data = pd.read_csv(dir + 'data.csv')\n",
    "\n",
    "class CustomDatasetFromImages(Dataset):\n",
    "    def __init__(self, csv_path, transformation):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_path (string): path to csv file\n",
    "            img_path (string): path to the folder where images are\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        # Transforms\n",
    "        self.to_tensor = transformation\n",
    "        # Read the csv file\n",
    "        self.data_info = pd.read_csv(csv_path, header=None)\n",
    "        # First column contains the image paths\n",
    "        self.image_arr = np.asarray(dir + 'png-i/' + self.data_info.iloc[:, 6] + '_alff.nii.jpg')\n",
    "        # Second column is the labels\n",
    "        self.label_arr = np.asarray(self.data_info.iloc[:, 7])\n",
    "        # Third column is for an operation indicator\n",
    "        # self.operation_arr = np.asarray(self.data_info.iloc[:, 2])\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.data_info.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_name = self.image_arr[index]\n",
    "        # Open image\n",
    "        img_as_img = Image.open(single_image_name)\n",
    "\n",
    "        # Check if there is an operation\n",
    "        #some_operation = self.operation_arr[index]\n",
    "        # If there is an operation\n",
    "        #if some_operation:\n",
    "            # Do some operation on image\n",
    "            # ...\n",
    "            # ...\n",
    "        #    pass\n",
    "        # Transform image to tensor\n",
    "        img_as_tensor = self.to_tensor(img_as_img)\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        single_image_label = self.label_arr[index]\n",
    "\n",
    "        return (img_as_tensor, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "#     transformations = transforms.Compose([\n",
    "#             transforms.RandomSizedCrop(224),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.ToTensor(),\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dataset_train = CustomDatasetFromImages(dir + 'train.csv', transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "\n",
    "dataset_test = CustomDatasetFromImages(dir + 'test.csv', transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    \n",
    "# Data loader\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                         shuffle=True)\n",
    "\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "#                                          batch_size=batch_size, \n",
    "#                                          shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "# dataset_size = len(dataset)\n",
    "# indices = list(range(dataset_size))\n",
    "# split = int(np.floor(validation_split * dataset_size))\n",
    "# if shuffle_dataset :\n",
    "#     np.random.seed(random_seed)\n",
    "#     np.random.shuffle(indices)\n",
    "# train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "# train_sampler = SubsetRandomSampler(train_indices)\n",
    "# validation_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, \n",
    "                                           num_workers=workers, pin_memory=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True,\n",
    "                                                num_workers=workers, pin_memory=True)\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"Train the model on Training Set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "    \n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        #topk = (1,5) if labels >= 100 else (1,) # TO FIX\n",
    "        # For nets that have multiple outputs such as Inception\n",
    "        if isinstance(output, tuple):\n",
    "            loss = sum((criterion(o,target_var) for o in output))\n",
    "            # print (output)\n",
    "            for o in output:\n",
    "                prec1 = accuracy(o.data, target, topk=(1,))\n",
    "                top1.update(prec1[0], input.size(0))\n",
    "            losses.update(loss.data[0], input.size(0)*len(output))\n",
    "        else:\n",
    "            loss = criterion(output, target_var)\n",
    "            prec1 = accuracy(output.data, target, topk=(1,))\n",
    "            top1.update(prec1[0], input.size(0))\n",
    "            losses.update(loss.data[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # Info log every args.print_freq\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1_val} ({top1_avg})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   top1_val=np.asscalar(top1.val.cpu().numpy())+0XA,\n",
    "                   top1_avg=np.asscalar(top1.avg.cpu().numpy())+0XA))\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"Validate the model on Validation Set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    # Evaluate all the validation set\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "    \n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        # print (\"Output: \", output)\n",
    "        #topk = (1,5) if labels >= 100 else (1,) # TODO: add more topk evaluation\n",
    "        # For nets that have multiple outputs such as Inception\n",
    "        if isinstance(output, tuple):\n",
    "            loss = sum((criterion(o,target_var) for o in output))\n",
    "            # print (output)\n",
    "            for o in output:\n",
    "                prec1 = accuracy(o.data, target, topk=(1,))\n",
    "                top1.update(prec1[0], input.size(0))\n",
    "            losses.update(loss.data[0], input.size(0)*len(output))\n",
    "        else:\n",
    "            loss = criterion(output, target_var)\n",
    "            prec1 = accuracy(output.data, target, topk=(1,))\n",
    "            top1.update(prec1[0], input.size(0))\n",
    "            losses.update(loss.data[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # Info log every args.print_freq\n",
    "        if i % print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1_val} ({top1_avg})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time,\n",
    "                   loss=losses,\n",
    "                   top1_val=np.asscalar(top1.val.cpu().numpy())+0XA,\n",
    "                   top1_avg=np.asscalar(top1.avg.cpu().numpy())+0XA))\n",
    "\n",
    "    print(' * Prec@1 {top1}'\n",
    "          .format(top1=np.asscalar(top1.avg.cpu().numpy())+0XA))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def test(test_loader, model, classes):\n",
    "    \"\"\"Test the model on the Evaluation Folder\n",
    "    Args:\n",
    "        - classes: is a list with the class name\n",
    "        - names: is a generator to retrieve the filename that is classified\n",
    "    \"\"\"\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    # Evaluate all the validation set\n",
    "    for i, (input, _) in enumerate(test_loader):\n",
    "    \n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        # Take last layer output\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[len(output)-1]\n",
    "\n",
    "        # print (output.data.max(1, keepdim=True)[1])\n",
    "        lab = classes[np.asscalar(output.data.max(1, keepdim=True)[1].cpu().numpy())]\n",
    "        print (\"Image classified as: \" + lab)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = learning_rate * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "if pretrained:\n",
    "        print(\"=> using pre-trained model '{}'\".format(arch))\n",
    "        model = models.__dict__[arch](pretrained=True)\n",
    "        print(model)\n",
    "        # quit()\n",
    "else:\n",
    "        print(\"=> creating model '{}'\".format(arch))\n",
    "        model = models.__dict__[arch](num_classes=num_classes)\n",
    "        # print(model)\n",
    "\n",
    "    # Freeze model, train only the last FC layer for the transfered task\n",
    "        if fine_tune:\n",
    "            print(\"=> transfer-learning mode + fine-tuning (train only the last FC layer)\")\n",
    "            # Freeze Previous Layers(now we are using them as features extractor)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Fine Tuning the last Layer For the new task\n",
    "            model.classifier._modules['6'] = nn.Linear(4096, num_classes+1)\n",
    "            parameters = model.classifier._modules['6'].parameters()\n",
    "            print(model)\n",
    "            # quit()\n",
    "\n",
    "        else:\n",
    "            parameters = model.parameters()\n",
    "\n",
    "    \n",
    "# Define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set SGD + Momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "# optionally resume from a checkpoint\n",
    "\n",
    "# Load model on CPU\n",
    "model.cpu()\n",
    "\n",
    "############ TRAIN/EVAL/TEST ############\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Training\n",
    "\n",
    "print(\"=> training...\")\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # Train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    prec1 = validate(validation_loader, model, criterion)\n",
    "    # print (prec1)\n",
    "\n",
    "    #Test data\n",
    "    #names = get_images_name(os.path.join(testdir, 'images'))\n",
    "    #test(validation_loader, model, train_dataset.classes)\n",
    "\n",
    "    # Remember best prec@1 and save checkpoint\n",
    "    prec1 = prec1.cpu() # Load on CPU if CUDA\n",
    "    # Get bool not ByteTensor\n",
    "    is_best = prec1 > best_prec1\n",
    "    # Get greater Tensor\n",
    "    best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
